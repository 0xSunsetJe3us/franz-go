package kgo

import "time"

// RecordHeader contains extra information that can be sent with Records.
type RecordHeader struct {
	Key   string
	Value []byte
}

type RecordAttrs struct {
	// 6 bits are used right now for record batches, and we use the high
	// bit to signify no timestamp due to v0 message set.
	//
	// bits 1 thru 3:
	//   000 no compression
	//   001 gzip
	//   010 snappy
	//   011 lz4
	//   100 zstd
	// bit 4: timestamp type
	// bit 5: is transactional
	// bit 6: is control
	// bit 8: no timestamp type
	attrs uint8
}

// TimestampType specifies how Timestamp was determined.
//
// The default, 0, means that the timestamp was determined in a client
// when the record was produced.
//
// An alternative is 1, which is when the Timestamp is set in Kafka.
//
// Records pre 0.10.0 did not have timestamps and have value -1.
func (a RecordAttrs) TimestampType() int8 {
	if a.attrs&0b1000_0000 != 0 {
		return -1
	}
	return int8(a.attrs & 0b0000_1000)
}

// CompressionType signifies with which algorithm this record was compressed.
//
// 0 is no compression, 1 is gzip, 2 is snappy, 3 is lz4, and 4 is zstd.
func (a RecordAttrs) CompressionType() uint8 {
	return a.attrs & 0b0000_0111
}

// IsTransactional returns whether a record is a part of a transaction.
func (a RecordAttrs) IsTransactional() bool {
	return a.attrs&0b0001_0000 != 0
}

// IsControl returns whether a record is a "control" record (ABORT or COMMIT).
// These are generally not visible unless explicitly opted into.
func (a RecordAttrs) IsControl() bool {
	return a.attrs&0b0010_0000 != 0
}

// Record is a record to write to Kafka.
type Record struct {
	// Key is an optional field that can be used for partition assignment.
	//
	// This is generally used with a hash partitioner to cause all records
	// with the same key to go to the same partition.
	Key []byte
	// Value is blob of data to write to Kafka.
	Value []byte

	// Headers are optional key/value pairs that are passed along with
	// records.
	//
	// These are purely for producers and consumers; Kafka does not look at
	// this field and only writes it to disk.
	Headers []RecordHeader

	// NOTE: if logAppendTime, timestamp is MaxTimestamp, not first + delta
	// zendesk/ruby-kafka#706

	// Timestamp is the timestamp that will be used for this record.
	//
	// Record batches are always written with "CreateTime", meaning that
	// timestamps are generated by clients rather than brokers.
	//
	// This field is always set in Produce.
	Timestamp time.Time

	// Attrs specifies what attributes were on this record.
	Attrs RecordAttrs

	// Topic is the topic that a record is written to.
	//
	// This must be set for producing.
	Topic string

	// Partition is the partition that a record is written to.
	//
	// For producing, this is left unset. If acks are required, this field
	// will be filled in before the produce callback if the produce is
	// successful.
	Partition int32

	// LeaderEpoch is the leader epoch of the broker at the time this
	// record was written, or -1 if on message sets.
	LeaderEpoch int32

	// Offset is the offset that a record is written as.
	//
	// For producing, this is left unset. If acks are required, this field
	// will be filled in before the produce callback if the produce is
	// successful.
	Offset int64
}

type FetchPartition struct {
	Partition        int32
	Err              error
	HighWatermark    int64
	LastStableOffset int64
	Records          []*Record
}

type FetchTopic struct {
	Topic      string
	Partitions []FetchPartition
}

type Fetch struct {
	Topics []FetchTopic
}

type Fetches []Fetch

func (fs Fetches) RecordIter() *FetchesRecordIter {
	iter := &FetchesRecordIter{fetches: fs}
	iter.prepareNext()
	return iter
}

type FetchesRecordIter struct {
	fetches []Fetch
}

func (i *FetchesRecordIter) Done() bool {
	return len(i.fetches) == 0
}

func (i *FetchesRecordIter) Next() *Record {
	records := &i.fetches[0].Topics[0].Partitions[0].Records
	next := (*records)[0]
	*records = (*records)[1:]
	i.prepareNext()
	return next
}

func (i *FetchesRecordIter) prepareNext() {
beforeFetch0:
	if len(i.fetches) == 0 {
		return
	}

beforeTopic0:
	fetch0 := &i.fetches[0]
	if len(fetch0.Topics) == 0 {
		i.fetches = i.fetches[1:]
		goto beforeFetch0
	}

beforePartition0:
	topic0 := &fetch0.Topics[0]
	if len(topic0.Partitions) == 0 {
		fetch0.Topics = fetch0.Topics[1:]
		goto beforeTopic0
	}

	partition0 := &topic0.Partitions[0]
	if len(partition0.Records) == 0 {
		topic0.Partitions = topic0.Partitions[1:]
		goto beforePartition0
	}
}
