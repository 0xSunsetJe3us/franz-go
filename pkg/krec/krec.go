// Package krec contains the basic Kafka record type that we use throughout our
// client. This is a dedicated package to effectively be a base point that
// other internal packages can use without creating a cycle with the kgo client
// package.
package krec

import "time"

// Header contains extra information that can be sent with records.
type Header struct {
	Key   string
	Value []byte
}

// Attrs contains attribute information about records and the batch they came
// in.
//
// Records themselves have an 8 bit attribute field that is currently
// completely unused; in this type, those attrs correspond to bits 17 through
// 24. The first 16 bits are the attributes of the record batch. Bit 25 is set
// if the record is translated from a v0 message set record, which contains no
// timestamp type. In record batches, only 6 bytes are currently used. Thus,
// for this type, only 7 bits truly signal information.
//
// bits 1 thru 3:
//   000 no compression
//   001 gzip
//   010 snappy
//   011 lz4
//   100 zstd
// bit  4: timestamp type
// bit  5: is transactional
// bit  6: is control
// bit 25: no timestamp type
type Attrs uint32

// UnknownTimestampBit is the bit that is set in Attrs if a record is from a v0
// message set.
const UnknownTimestampBit = 0x01000000

// TimestampType specifies how Timestamp was determined.
//
// The default, 0, means that the timestamp was determined in a client
// when the record was produced.
//
// An alternative is 1, which is when the Timestamp is set in Kafka.
//
// Records pre 0.10.0 did not have timestamps and have value -1.
func (a Attrs) TimestampType() int8 {
	if a&UnknownTimestampBit != 0 {
		return -1
	}
	return int8(a & 0x00000008)
}

// CompressionType signifies with which algorithm this record was compressed.
//
// 0 is no compression, 1 is gzip, 2 is snappy, 3 is lz4, and 4 is zstd.
func (a Attrs) CompressionType() uint8 {
	return uint8(a & 0x00000007)
}

// IsTransactional returns whether a record is a part of a transaction.
func (a Attrs) IsTransactional() bool {
	return a&0x00000010 != 0
}

// IsControl returns whether a record is a "control" record (ABORT or COMMIT).
// These are generally not visible unless explicitly opted into.
func (a Attrs) IsControl() bool {
	return a&0x00000020 != 0
}

// Rec is a record to write to Kafka.
type Rec struct {
	// Key is an optional field that can be used for partition assignment.
	//
	// This is generally used with a hash partitioner to cause all records
	// with the same key to go to the same partition.
	Key []byte
	// Value is blob of data to write to Kafka.
	Value []byte

	// Headers are optional key/value pairs that are passed along with
	// records.
	//
	// These are purely for producers and consumers; Kafka does not look at
	// this field and only writes it to disk.
	Headers []Header

	// NOTE: if logAppendTime, timestamp is MaxTimestamp, not first + delta
	// zendesk/ruby-kafka#706

	// Timestamp is the timestamp that will be used for this record.
	//
	// Record batches are always written with "CreateTime", meaning that
	// timestamps are generated by clients rather than brokers.
	//
	// This field is always set in Produce.
	Timestamp time.Time

	// Attrs specifies what attributes were on this record.
	Attrs Attrs

	// Topic is the topic that a record is written to.
	//
	// This must be set for producing.
	Topic string

	// Partition is the partition that a record is written to.
	//
	// For producing, this is left unset. If acks are required, this field
	// will be filled in before the produce callback if the produce is
	// successful.
	Partition int32

	// LeaderEpoch is the leader epoch of the broker at the time this
	// record was written, or -1 if on message sets.
	LeaderEpoch int32

	// Offset is the offset that a record is written as.
	//
	// For producing, this is left unset. If acks are required, this field
	// will be filled in before the produce callback if the produce is
	// successful.
	Offset int64
}
