// Header is user provided metadata for a record. Kafka does not look at
// headers at all; they are solely for producers and consumers.
Header => not top level
  Key: varint-string
  Value: varint-bytes

// A Record is a Kafka v0.11.0.0 record. It corresponds to an individual
// message as it is written on the wire.
Record => not top level
  // Length is the length of this record on the wire of everything that
  // follows this field. It is an int32 encoded as a varint.
  Length: varint
  // Attributes are record level attributes. This field currently is unused.
  Attributes: int8
  // TimestampDelta is the millisecond delta of this record's timestamp
  // from the record's RecordBatch's FirstTimestamp.
  TimestampDelta: varint
  // OffsetDelta is the delta of this record's offset from the record's
  // RecordBatch's FirstOffset.
  //
  // For producing, this is usually equal to the index of the record in
  // the record batch.
  OffsetDelta: varint
  // Key is an blob of data for a record.
  //
  // Key's are usually used for hashing the record to specific Kafka partitions.
  Key: varint-bytes
  // Value is  a blob of data. This field is the main "message" portion of a
  // record.
  Value: varint-bytes
  // Headers are optional user provided metadata for records. Unlike normal
  // arrays, the number of headers is encoded as a varint.
  Headers: varint[Header]

// RecordBatch is a Kafka concept that groups many individual records together
// in a more optimized format.
RecordBatch => not top level
  // NullableBytesLength is not officially a field in a RecordBatch, however,
  // RecordBatch is a special form of NULLABLE_BYTES. Since all nullable bytes
  // must be prefixed with a int32 length, we throw that here.
  // This length should not include itself, only data that follows.
  NullableBytesLength: int32
  // FirstOffset is the first offset in a record batch.
  //
  // For producing, this is usually 0.
  FirstOffset: int64
  // Length is the wire length of everything that follows this field.
  Length: int32
  // PartitionLeaderEpoch is a number that Kafka uses for cluster
  // communication. Clients generally do not need to worry about this
  // field and producers should set it to -1.
  PartitionLeaderEpoch: int32
  // Magic is the current "magic" number of this message format.
  // The current magic number is 2.
  Magic: int8
  // CRC is the crc of everything that follows this field using the
  // Castagnoli polynomial.
  CRC: int32
  // Attributes describe the records array of this batch.
  //
  // Bits 0 thru 3 correspond to compression:
  //   - 000 is no compression
  //   - 001 is snappy compression
  //   - 010 is lz4 compression
  //   - 011 is zstd compression (produce request version 7+)
  //
  // Bit 4 is the timestamp type, with 0 meaning CreateTime corresponding
  // to the timestamp being from the producer, and 1 meaning LogAppendTime
  // corresponding to the timestamp being from the broker.
  // Setting this to LogAppendTime will cause batches to be rejected.
  //
  // Bit 5 indicates whether the batch is part of a transaction (1 is yes).
  //
  // Bit 6 indicates if the batch includes a control message (1 is yes).
  // Control messages are used to enable transactions and are generated from
  // the broker. Clients should not return control batches to applications.
  Attributes: int16
  // LastOffsetDelta is the offset of the last message in a batch. This is
  // by the broker to ensure correct behavior even with batch compaction.
  LastOffsetDelta: int32
  // FirstTimestamp is the timestamp (in milliseconds) of the first record
  // in a batch.
  FirstTimestamp: int64
  // MaxTimestamp is the timestamp (in milliseconds) of the last record
  // in a batch. Similar to LastOffsetDelta, this is used to ensure correct
  // behavior with compacting.
  MaxTimestamp: int64
  // ProducerID is the broker assigned producerID from an InitProducerID
  // request.
  //
  // Clients that wish to support idempotent messages and transactions must
  // set this field.
  ProducerID: int64
  // ProducerEpoch is the broker assigned producerEpoch from an InitProducerID
  // request.
  //
  // Clients that wish to support idempotent messages and transactions must
  // set this field.
  ProducerEpoch: int16
  // FirstSequence is the producer assigned sequence number used by the
  // broker to deduplicate messages.
  //
  // Clients that wish to support idempotent messages and transactions must
  // set this field.
  //
  // The sequence number for each record in a batch is OffsetDelta + FirstSequence.
  FirstSequence: int32
  // Records are the batch of records to send.
  //
  // Note that to compress a batch, you compress the entire set of records
  // and then use that as the value for a single record.
  Records: [Record]

// ProduceRequest issues records to be created to Kafka.
//
// The min version of this type is currently 2, released with Kafka 0.11.0.0.
// Prior, the RecordBatch format was completely different (and was called a
// MessageSet).
//
// Note that the special client ID "__admin_client" will allow you to produce
// records to internal topics. This is generally recommended if you want to
// break your Kafka cluster.
ProduceRequest => key 0, max version 7, min version 2
  TransactionID: nullable-string // v3+
  // Acks specifies the number of acks that the partition leaders must receive
  // from in sync replicas before considering a record batch fully written.
  //
  // Valid values are -1, 0, or 1 corresponding to all, none, or one.
  //
  // Note that if no acks are requested, Kafka will close the connection
  // if any topic or partition errors to trigger a client metadata refresh.
  Acks: int16
  // Timeout is the millisecond timeout of this request.
  Timeout: int32
  // TopicData is an array of topics to send record batches to.
  TopicData: [=>]
    // Topic is a topic to send record batches to.
    Topic: string
    // Data is an array of partitions to send record batches to.
    Data: [=>]
      // Partition is a partition to send a record batch to.
      Partition: int32
      // Records is a batch of records to write to a topic's partition.
      Records: RecordBatch

// ProduceResponse is returned from a ProduceRequest.
ProduceResponse => from ProduceRequest
  // Responses is an array of responses for the topic's that batches were sent
  // to.
  Responses: [=>]
    // Topic is the topic this response pertains to.
    Topic: string
    // PartitionResponses is an array of responses for the partition's that
    // batches were sent to.
    PartitionResponses: [=>]
      // Partition is the partition this response pertains to.
      Partition: int32
      // ErrorCode is any error for a topic/partition in the request.
      // There are many error codes for produce requests.
      //
      // TRANSACTIONAL_ID_AUTHORIZATION_FAILED is returned for all topics and
      // partitions if the request had a transactional ID but the client
      // is not authorized for transactions.
      //
      // CLUSTER_AUTHORIZATION_FAILED is returned for all topics and partitions
      // if the request was idempotent but the client is not authorized
      // for idempotent requests.
      //
      // TOPIC_AUTHORIZATION_FAILED is returned for all topics the client
      // is not authorized to talk to.
      //
      // INVALID_REQUIRED_ACKS is returned if the request contained an invalid
      // number for "acks".
      //
      // CORRUPT_MESSAGE is returned for many reasons, generally related to
      // problems with messages (invalid magic, size mismatch, etc.).
      //
      // MESSAGE_TOO_LARGE is returned if a record batch is larger than the
      // broker's configured max.message.size.
      //
      // RECORD_LIST_TOO_LARGE is returned if the record batch is larger than
      // the broker's segment.bytes.
      //
      // INVALID_TIMESTAMP is returned if the record batch uses LogAppendTime
      // or if the timestamp delta from when the broker receives the message
      // is more than the broker's log.message.timestamp.difference.max.ms.
      //
      // UNSUPPORTED_FOR_MESSAGE_FORMAT is returned if using a Kafka v2 message
      // format (i.e. RecordBatch) feature (idempotence) while sending v1
      // messages (i.e. a MessageSet).
      //
      // KAFKA_STORAGE_ERROR is returned if the log directory for a partition
      // is offline.
      //
      // NOT_ENOUGH_REPLICAS is returned if all acks are required, but there
      // are not enough in sync replicas yet.
      //
      // NOT_ENOUGH_REPLICAS_AFTER_APPEND is returned on old Kafka versions
      // (pre 0.11.0.0) when a message was written to disk and then Kafka
      // noticed not enough replicas existed to replicate the message.
      //
      // DUPLICATE_SEQUENCE_NUMBER is returned for Kafka <1.1.0 when a
      // sequence number is detected as a duplicate. After, out of order
      // is returned.
      //
      // UNKNOWN_TOPIC_OR_PARTITION is returned if the topic or partition
      // is unknown.
      //
      // NOT_LEADER_FOR_PARTITION is returned if the broker is not a leader
      // for this partition. This means that the client has stale metadata.
      //
      // INVALID_PRODUCER_EPOCH is returned if the produce request was
      // attempted with an old epoch. Either there is a newer producer using
      // the same transaction ID, or the transaction ID used has expired.
      //
      // UNKNOWN_PRODUCER_ID, added in Kafka 1.0.0 (message format v5+) is
      // returned if the producer used an ID that Kafka does not know about.
      // The LogStartOffset must be checked in this case. If the offset is
      // greater than the last acknowledged offset, then no data loss has
      // occurred; the client just sent data so long ago that Kafka rotated
      // the partition out of existence and no longer knows of this producer
      // ID. In this case, initialize a new ID. If the log start offset is
      // equal to or less than what the client sent prior, then data loss
      // has occurred. This See KAFKA-5793 for more details.
      //
      // OUT_OF_ORDER_SEQUENCE_NUMBER is sent if the batch's FirstSequence was
      // not what it should be (the last FirstSequence, plus the number of
      // records in the last batch, plus one). After 1.0.0, this generally
      // means data loss. Before, there could be confusion on if the broker
      // actually rotated the partition out of existence (this is why
      // UNKNOWN_PRODUCER_ID was introduced).
      ErrorCode: int16
      // BaseOffset is the offset that the records in the produce request began
      // at in the partition.
      BaseOffset: int64
      // LogAppendTime is the time that records were appended to the partition
      // inside Kafka. This is only not -1 if records were written with the log
      // append time flag (which producers cannot do).
      LogAppendTime: int64 // v2+
      // LogStartOffset, introduced in Kafka 1.0.0,  can be used to see if an
      // UNKNOWN_PRODUCER_ID means Kafka rotated records containing the used
      // producer ID out of existence, or if Kafka lost data.
      LogStartOffset: int64 // v5+
  // ThrottleTimeMs is how long of a throttle Kafka will apply to the client
  // after this request.
  // For Kafka < 2.0.0, the throttle is applied before issuing a response.
  // For Kafka >= 2.0.0, the throttle is applied after issuing a response.
  ThrottleTimeMs: int32

FetchRequest => key 1, max version 10
  ReplicaID: int32
  MaxWaitTime: int32
  MinBytes: int32
  MaxBytes: int32 // v3+
  IsolationLevel: int8 // v4+
  SessionID: int32 // v7+
  SessionEpoch: int32 // v7+
  Topics: [=>]
    Topic: string
    Partitions: [=>]
      Partition: int32
      CurrentLeaderEpoch: int32 // v9+
      FetchOffset: int64
      LogStartOffset: int64 // v5+
      PartitionMaxBytes: int32
  ForgottenTopicsData: [=>]
    Topic: string
    Partitions: [int32]

FetchResponse => from FetchRequest
  ThrottleTimeMs: int32 // v1+
  ErrorCode: int16 // v7+
  SessionID: int32 // v7+
  Responses: [=>]
    Topic: string
    PartitionResponses: [=>]
      PartitionHeader: =>
        Partition: int32
        ErrorCode: int16
        HighWatermark: int64
        LastStableOffset: int64 // v4+
        LogStartOffset: int64 // v5+
        AbortedTransactions: [=>] // v4+
          ProducerID: int64 // v4+
          FirstOffset: int64 // v4+
      RecordSet: [Record]

ListOffsetsRequest => key 2, max version 4, min version 1, admin
  // ReplicaID is the broker ID to get offsets from. As a Kafka client, use -1.
  // The consumer replica ID (-1) causes requests to only succeed if issued
  // against the leader broker.
  ReplicaID: int32
  // IsolationLevel configures which record offsets are visible in the
  // response. READ_UNCOMMITTED (0) makes all records visible. READ_COMMITTED
  // (1) makes non-transactional and committed transactional records visible.
  // READ_COMMITTED means all offsets smaller than the last stable offset and
  // includes aborted transactions (allowing consumers to discard aborted
  // records).
  IsolationLevel: int8 // v2+
  // Topics is an array of topics to get offsets for.
  Topics: [=>]
    // Topic is a topic to get offsets for.
    Topic: string
    // Partitions is an array of partitions in a topic to get offsets for.
    Partitions: [=>]
      // Partition is a partition of a topic to get offsets for.
      Partition: int32
      // CurrentLeaderEpoch, proposed in KIP-320 and introduced in Kafka 2.1.0,
      // allows brokers to check if the client is fenced (has an out of date
      // leader) or is using an unknown leader.
      CurrentLeaderEpoch: int32 // v4+
      // Timestamp controls which offset to return in a response for this
      // partition.
      //
      // The offset returned will be the one of the message whose timestamp is
      // the first timestamp greater than or equal to this requested timestamp.
      //
      // If no such message is found, the log end offset is returned.
      //
      // There exist two special timestamps: -2 corresponds to the earliest
      // timestamp, and -1 corresponds to the latest.
      Timestamp: int64

// ListOffsetsResponse is returned from a ListOffsetsRequest.
ListOffsetsResponse => from ListOffsetsRequest
  // ThrottleTimeMs is how long of a throttle Kafka will apply to the client
  // after this request.
  // For Kafka < 2.0.0, the throttle is applied before issuing a response.
  // For Kafka >= 2.0.0, the throttle is applied after issuing a response.
  ThrottleTimeMs: int32 // v2+
  // Responses is an array of topic / partition responses corresponding to
  // the requested topics and partitions.
  Responses: [=>]
    // Topic is the topic this array slot is for.
    Topic: string
    // PartitionResponses is an array of partition responses corresponding to
    // the requested partitions for a topic.
    PartitionResponses: [=>]
      // Partition is the partition this array slot is for.
      Partition: int32
      // TOPIC_AUTHORIZATION_FAILED is returned if the client is not authorized
      // to describe the topic.
      //
      // INVALID_REQUEST is returned if the requested topic partitions had
      // contained duplicates.
      //
      // KAFKA_STORAGE_EXCEPTION is returned if the topic / partition is in
      // an offline log directory.
      //
      // UNSUPPORTED_FOR_MESSAGE_FORMAT is returned if the broker is using
      // Kafka 0.10.0 messages and the requested timestamp was not -1 nor -2.
      //
      // NOT_LEADER_FOR_PARTITION is returned if the broker is not a leader
      // for this partition. This means that the client has stale metadata.
      // If the request used the debug replica ID, the returned error will
      // be REPLICA_NOT_AVAILABLE.
      //
      // UNKNOWN_TOPIC_OR_PARTITION is returned if the broker does not know
      // of the requested topic or partition.
      //
      // FENCED_LEADER_EPOCH is returned if the broker has a higher leader
      // epoch than what the request sent.
      //
      // UNKNOWN_LEADER_EPOCH is returned if the request used a leader epoch
      // that the broker does not know about.
      //
      // OFFSET_NOT_AVAILABLE, introduced in Kafka 2.2.0 with produce request
      // v5+, is returned when talking to a broker that is a new leader while
      // that broker's high water mark catches up. This avoids situations where
      // the old broker returned higher offsets than the new broker would. Note
      // that if unclean leader election is allowed, you could still run into
      // the situation where offsets returned from list offsets requests are
      // not monotonically increasing. This error is only returned if the
      // request used the consumer replica ID (-1). If the client did not use
      // a v5+ list offsets request, LEADER_NOT_AVAILABLE is returned.
      // See KIP-207 for more details.
      ErrorCode: int16
      // If the request was for the earliest or latest timestamp (-2 or -1), or
      // if an offset could not be found after the requested one, this will be -1.
      Timestamp: int64
      // Offset is the offset corresponding to the record on or after the
      // requested timestamp. If one could not be found, this will be -1.
      Offset: int64 // v1+
      // LeaderEpoch is the leader epoch of the record at this offset,
      // or -1 if there was no leader epoch.
      LeaderEpoch: int32 // v4+

// MetadataRequest requests metadata from Kafka.
MetadataRequest => key 3, max version 8
  // Topics is a list of topics to return metadata about. If this is null,
  // all topics are included. If this is empty, no topics are.
  // For v0 (<Kafka 0.10.0.0), if this is empty, all topics are included.
  Topics: [string]
  // AllowAutoTopicCreation, introduced in Kafka 0.11.0.0, allows topic
  // auto creation of the topics in this request if they do not exist.
  AllowAutoTopicCreation: bool // v4+
  // IncludeClusterAuthorizedOperations, introduced in Kakfa 2.3.0, specifies
  // whether to return a bitfield of AclOperations that this client can perform
  // on the cluster.
  IncludeClusterAuthorizedOperations: bool // v8+
  // IncludeTopicAuthorizedOperations, introduced in Kakfa 2.3.0, specifies
  // whether to return a bitfield of AclOperations that this client can perform
  // on individual topics.
  IncludeTopicAuthorizedOperations: bool // v8+

// MetadataResponse is returned from a MetdataRequest.
MetadataResponse => from MetadataRequest
  // ThrottleTimeMs is how long of a throttle Kafka will apply to the client
  // after this request.
  // For Kafka < 2.0.0, the throttle is applied before issuing a response.
  // For Kafka >= 2.0.0, the throttle is applied after issuing a response.
  ThrottleTimeMs: int32 // v3+
  // Brokers is a set of alive Kafka brokers.
  Brokers: [=>]
    // NodeID is the node ID of a Kafka broker.
    NodeID: int32
    // Host is the hostname of a Kafka broker.
    Host: string
    // Port is the port of a Kafka broker.
    Port: int32
    // Rack is the rack this Kafka broker is in.
    Rack: nullable-string // v1+
  // ClusterID, proposed in KIP-78 and introduced in Kafka 0.10.1.0, is a
  // unique string specifying the cluster that the replying Kafka belongs to.
  ClusterID: nullable-string // v2+
  // ControllerID is the ID of the controller broker (the admin broker).
  ControllerID: int32 // v1+
  // TopicMetadata contains metadata about each topic requested in the
  // MetadataRequest.
  TopicMetadata: [=>]
    // ErrorCode is any error for a topic in a metadata request.
    //
    // TOPIC_AUTHORIZATION_FAILED is returned if the client is not authorized
    // to describe the topic, or if the metadata request specified topic auto
    // creation, the topic did not exist, and the user lacks permission to create.
    //
    // UNKNOWN_TOPIC_OR_PARTITION is returned if a topic does not exist and
    // the request did not specify autocreation.
    //
    // LEADER_NOT_AVAILABLE is returned if a new topic is created successfully
    // (since there is no leader on an immediately new topic).
    //
    // There can be a myriad of other errors for unsuccessful topic creation.
    ErrorCode: int16
    // Topic is the topic this metadata corresponds to.
    Topic: string
    // IsInternal signifies whether this topic is a Kafka internal topic.
    IsInternal: bool // v1+
    // PartitionMetadata contains metadata about partitions for a topic.
    PartitionMetadata: [=>]
      // ErrorCode is any error for a partition in topic metadata.
      //
      // LEADER_NOT_AVAILABLE is returned if a leader is unavailable for this
      // partition. For v0 metadata responses, this is also returned if a
      // partition leader's listener does not exist.
      //
      // LISTENER_NOT_FOUND is returned if a leader ID is known but the
      // listener for it is not (v1+).
      //
      // REPLICA_NOT_AVAILABLE is returned in v0 responses if any replica is
      // unavailable.
      ErrorCode: int16
      // Partition is a partition number for a topic.
      Partition: int32
      // Leader is the broker leader for this partition. This will be -1
      // on leader / listener error.
      Leader: int32
      // LeaderEpoch, proposed in KIP-320 and introduced in Kafka 2.1.0  is the
      // epoch of the broker leader.
      LeaderEpoch: int32 // v7+
      // Replicas returns all broker IDs containing replicas of this partition.
      Replicas: [int32]
      // ISR returns all broker IDs of in-sync replicas of this partition.
      ISR: [int32]
      // OfflineReplicas, proposed in KIP-112 and introduced in Kafka 1.0,
      // returns all offline broker IDs that should be replicating this partition.
      OfflineReplicas: [int32] // v5+
    // AuthorizedOperations, proposed in KIP-430 and introduced in Kafka 2.3.0,
    // returns a bitfield (corresponding to AclOperation) containing which
    // operations the client is allowed to perform on this topic.
    // This is only returned if requested.
    AuthorizedOperations: int32 // v8+
  // AuthorizedOperations returns a bitfield containing which operations the
  // client is allowed to perform on this cluster.
  AuthorizedOperations: int32 // v8+

LeaderAndISRRequest => key 4, max version 1, admin
  ControllerID: int32
  ControllerEpoch: int32
  PartitionStates: [=>]
    Topic: string
    Partition: int32
    ControllerEpoch: int32
    Leader: int32
    LeaderEpoch: int32
    ISR: [int32]
    ZKVersion: int32
    Replicas: [int32]
    IsNew: bool
  LiveLeaders: [=>]
    ID: int32
    Host: string
    Port: int32

LeaderAndISRResponse => from LeaderAndISRRequest
  ErrorCode: int16
  Partitions: [=>]
    Topic: string
    Partition: int32
    ErrorCode: int16

StopReplicaRequest => key 5, max version 0, admin
  ControllerID: int32
  ControllerEpoch: int32
  DeletePartitions: bool
  Partitions: [=>]
    Topic: string
    Partition: int32

StopReplicaResponse => from StopReplicaRequest
  ErrorCode: int16
  Partitions: [=>]
    Topic: string
    Partition: int32
    ErrorCode: int16

// V1 switched LiveBrokers struct type
UpdateMetadataRequest => key 6, max version 4, min version 1, admin
  ControllerID: int32
  ControllerEpoch: int32
  PartitionStates: [=>]
    Topic: string
    Partition: int32
    ControllerEpoch: int32
    Leader: int32
    LeaderEpoch: int32
    ISR: [int32]
    ZKVersion: int32
    Replicas: [int32]
    OfflineReplicas: [int32]
  LiveBrokers: [=>]
    ID: int32
    Endpoints: [=>] // v1+
      Port: int32 // v1+
      Host: string // v1+
      ListenerName: string // v3+
      SecurityProtocolType: int16 // v1+
    Rack: nullable-string // v2+

UpdateMetadataResponse => from UpdateMetadataRequest
  ErrorCode: int16

ControlledShutdownRequest => key 7, max version 1, admin
  BrokerID: int32

ControlledShutdownResponse => from ControlledShutdownRequest
  ErrorCode: int16
  PartitionsRemaining: [=>]
    Topic: string
    Partition: int32

OffsetCommitRequest => key 8, max version 6
  GroupID: string
  GenerationID: int32 // v1+
  MemberID: string // v1+
  RetentionTime: int64 // v2-v4
  Topics: [=>]
    Topic: string
    Partitions: [=>]
      Partition: int32
      Offset: int64
      Timestamp: int64 // v1-v1
      LeaderEpoch: int32 // v6+
      Metadata: nullable-string

OffsetCommitResponse => from OffsetCommitRequest
  ThrottleTimeMs: int32 // v3+
  Responses: [=>]
    Topic: string
    PartitionResponses: [=>]
      Partition: int32
      ErrorCode: int16

// ApiVersionsRequest requests what API versions a Kafka broker supports.
//
// Because a client does not know what version of ApiVersionsRequest a broker
// supports, Kafka responds to versions with the highest version it supports.
// This allows clients to always use the latest version of ApiVersionsRequest.
// If the broker supports only a lower version of the request, it will reply
// with an UNSUPPORTED_VERSION error.
ApiVersionsRequest => key 18, max version 2

// ApiVersionsResponse is returned from an ApiVersionsRequest.
ApiVersionsResponse => from ApiVersionsRequest
  // ErrorCode is UNSUPPORTED_VERSION if the request was issued with a higher
  // version than the broker supports. Regardless of the error, the broker
  // replies with the ApiVersions it supports.
  ErrorCode: int16
  // ApiVersions is an array corresponding to API keys the broker supports
  // and the range of supported versions for each key.
  ApiVersions: [=>]
    // ApiKey is the key of a message request.
    ApiKey: int16
    // MinVersion is the min version a broker supports for an API key.
    MinVersion: int16
    // MaxVersion is the max version a broker supports for an API key.
    MaxVersion: int16
  // ThrottleTimeMs is how long of a throttle Kafka will apply to the client
  // after this request.
  // For Kafka < 2.0.0, the throttle is applied before issuing a response.
  // For Kafka >= 2.0.0, the throttle is applied after issuing a response.
  ThrottleTimeMs: int32 // v1+

// DeleteTopicsRequest deletes a Kafka topic.
DeleteTopicsRequest => key 20, max version 3, admin
  // Topics is an array of topics to delete.
  Topics: [string]
  // Timeout is the millisecond timeout of this request.
  Timeout: int32

// DeleteTopicsResponse is returned from a DeleteTopicsRequest.
// Version 3 added the TOPIC_DELETION_DISABLED error proposed in KIP-322
// and introduced in Kafka 2.1.0. Prior, the request timed out.
DeleteTopicsResponse => from DeleteTopicsRequest
  // ThrottleTimeMs is how long of a throttle Kafka will apply to the client
  // after this request.
  // For Kafka < 2.0.0, the throttle is applied before issuing a response.
  // For Kafka >= 2.0.0, the throttle is applied after issuing a response.
  ThrottleTimeMs: int32 // v1+
  // TopicErrorCodes is contains the error codes for each topic requested
  // for deletion (or no error code).
  TopicErrorCodes: [=>]
    // Topic is the topic requested for deletion.
    Topic: string
    // ErrorCode is the error code returned for an individual topic in
    // deletion request.
    //
    // TOPIC_AUTHORIZATION_FAILED is returned if the client is not authorized
    // to delete a topic.
    //
    // UNKNOWN_TOPIC_OR_PARTITION is returned if the broker does not know of
    // the topic.
    //
    // NOT_CONTROLLER is returned if the request was not issued to a Kafka
    // controller.
    //
    // TOPIC_DELETION_DISABLED is returned for deletion requests version 3+
    // and brokers >= 2.1.0. INVALID_REQUEST is issued for request versions
    // 0-2 against brokers >= 2.1.0. Otherwise, the request hangs until it
    // times out.
    ErrorCode: int16

InitProducerIDRequest => key 22, max version 2
  TransactionalID: nullable-string
  TransactionTimeoutMs: int32

InitProducerIDResponse => from InitProducerIDRequest
  // ThrottleTimeMs is how long of a throttle Kafka will apply to the client
  // after this request.
  // For Kafka < 2.0.0, the throttle is applied before issuing a response.
  // For Kafka >= 2.0.0, the throttle is applied after issuing a response.
  ThrottleTimeMs: int32
  // CLUSTER_AUTHORIZATION_FAILED if idempotent and not authed
  //
  // transactional errors:
  // TRANSACTIONAL_ID_AUTHORIZATION_FAILED if transactional and not authed
  // INVALID_REQUEST if transactional id is an empty, non-null string
  // INVALID_TRANSACTION_TIMEOUT if timeout equal to over over transaction.max.timeout.ms or under 0
  // COORDINATOR_LOAD_IN_PROGRESS
  // NOT_COORDINATOR
  // COORDINATOR_NOT_AVAILABLE
  // CONCURRENT_TRANSACTIONS
  ErrorCode: int16
  // the generated producer ID
  ProducerID: int64
  // the current producer epoch
  ProducerEpoch: int16

DescribeLogDirsRequest => key 35, max version 1, admin
  Topics: [=>]
    Topic: string
    Partitions: [int32]

DescribeLogDirsResponse => from DescribeLogDirsRequest
  ThrottleTimeMs: int32
  LogDirs: [=>]
    ErrorCode: int16
    LogDir: string
    Topics: [=>]
      Topic: string
      Partitions: [=>]
        Partition: int32
        Size: int64
        OffsetLag: int64
        IsFuture: bool
